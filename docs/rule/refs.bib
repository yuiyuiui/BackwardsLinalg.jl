@article{Francuz2023,
  title={Stable and efficient differentiation of tensor network algorithms},
  author={Francuz, Anna and Schuch, Norbert and Vanhecke, Bram},
  journal={arXiv preprint arXiv:2311.11894},
  year={2023},
  url={https://arxiv.org/abs/2311.11894}
}

@inproceedings{Moses2021,
  title={Reverse-mode automatic differentiation and optimization of GPU kernels via Enzyme},
  author={Moses, William S and Churavy, Valentin and Paehler, Ludger and H{\"u}ckelheim, Jan and Narayanan, Sri Hari Krishna and Schanen, Michel and Doerfert, Johannes},
  booktitle={Proceedings of the international conference for high performance computing, networking, storage and analysis},
  pages={1--16},
  year={2021},
  url={https://dl.acm.org/doi/abs/10.1145/3458817.3476165}
}

@software{Jax2018,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{Paszke2019,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019},
  url={https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html}
}

@article{Hascoet2013,
  title={The Tapenade automatic differentiation tool: principles, model, and specification},
  author={Hascoet, Laurent and Pascual, Val{\'e}rie},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={39},
  number={3},
  pages={1--43},
  year={2013},
  publisher={ACM New York, NY, USA},
  url={https://dl.acm.org/doi/abs/10.1145/2450153.2450158}
}

@article{Blondel2022,
  title={Efficient and modular implicit differentiation},
  author={Blondel, Mathieu and Berthet, Quentin and Cuturi, Marco and Frostig, Roy and Hoyer, Stephan and Llinares-L{\'o}pez, Felipe and Pedregosa, Fabian and Vert, Jean-Philippe},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={5230--5242},
  year={2022},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/hash/228b9279ecf9bbafe582406850c57115-Abstract-Conference.html}
}

@article{Plessix2006,
    author = {Plessix, R.-E.},
    title = "{A review of the adjoint-state method for computing the gradient of a functional with geophysical applications}",
    journal = {Geophysical Journal International},
    volume = {167},
    number = {2},
    pages = {495-503},
    year = {2006},
    month = {11},
    issn = {0956-540X},
    doi = {10.1111/j.1365-246X.2006.02978.x},
    url = {https://doi.org/10.1111/j.1365-246X.2006.02978.x},
    eprint = {https://academic.oup.com/gji/article-pdf/167/2/495/1492368/167-2-495.pdf},
}

@inproceedings{Chen2018,
 author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Ordinary Differential Equations},
 url = {https://proceedings.neurips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{Li2017,
  title = {The {{Tapenade Automatic Differentiation}} Tool: Principles, Model, and Specification},
  author = {Li, Jie and Wang, Zhe Long and Zhao, Hongyu and Gravina, Raffaele and Fortino, Giancarlo and Jiang, Yongmei and Tang, Kai},
  year = {2017},
  journal = {BodyNets International Conference on Body Area Networks},
  issn = {23103582},
  doi = {10.1145/0000000.0000000},
  abstract = {In this paper, from the perspective of human ergonomics, we analyze the movement of the joints in the process of human body movements, and we establish a dynamic model according to the human skeleton structure. On this basis, from the rigid body dynamics point of view, combined with the principle of inertial navigation, a body sensor network based on MEMS inertial sensors is built to capture human body motion in real time. On the basis of space trajectory of human body movement and traditional human motion solution strategy, a human motion solution strategy based on particle filter fusion solution is proposed to realize the prediction of human motion analysis. Therefore, we evaluate the performance of the designed system by comparing with the real motion. Finally, in order to verify the human motion data, the motion capture data verification platforms are established. Experimental results show that the proposed joint attitude solution algorithm can achieve a relatively smooth tracking effect and provides a certain reference value.},
  keywords = {Body sensor network,Inertial navigation,Motion capture,Particle filter},
  file = {/Users/liujinguo/Zotero/storage/VJ4C9MIR/Li et al_2017_The Tapenade Automatic Differentiation tool.pdf}
}

@book{Griewank2008,
  title = {Evaluating {{Derivatives}}},
  author = {Griewank, Andreas and Walther, Andrea},
  year = {2008},
  journal = {Evaluating Derivatives},
  doi = {10.1137/1.9780898717761},
  abstract = {Algorithmic, or automatic, differentiation (AD) is a growing area of theoretical research and software development concerned with the accurate and efficient evaluation of derivatives for function evaluations given as computer programs. The resulting derivative values are useful for all scientific computations that are based on linear, quadratic, or higher order approximations to nonlinear scalar or vector functions. AD has been applied in particular to optimization, parameter identification, nonlinear equation solving, the numerical integration of differential equations, and combinations of these. Apart from quantifying sensitivities numerically, AD also yields structural dependence information, such as the sparsity pattern and generic rank of Jacobian matrices. The field opens up an exciting opportunity to develop new algorithms that reflect the true cost of accurate derivatives and to use them for improvements in speed and reliability. This second edition has been updated and expanded to cover recent developments in applications and theory, including an elegant NP completeness argument by Uwe Naumann and a brief introduction to scarcity, a generalization of sparsity. There is also added material on checkpointing and iterative differentiation. To improve readability the more detailed analysis of memory and complexity bounds has been relegated to separate, optional chapters.The book consists of three parts: a stand-alone introduction to the fundamentals of AD and its software; a thorough treatment of methods for sparse problems; and final chapters on program-reversal schedules, higher derivatives, nonsmooth problems and iterative processes. Each of the 15 chapters concludes with examples and exercises. Audience: This volume will be valuable to designers of algorithms and software for nonlinear computational problems. Current numerical software users should gain the insight necessary to choose and deploy existing AD software tools to the best advantage.},
  isbn = {978-0-89871-659-7},
  file = {/Users/liujinguo/Zotero/storage/PF7YDDDC/Griewank_Walther_2008_Evaluating Derivatives.pdf}
}

@article{Xie2020,
  title = {Automatic Differentiation of Dominant Eigensolver and Its Applications in Quantum Physics},
  author = {Xie, Hao and Liu, Jin-Guo and Wang, Lei},
  year = {2020},
  month = jun,
  journal = {Physical Review B},
  volume = {101},
  number = {24},
  pages = {245139},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.101.245139},
  urldate = {2023-03-23},
  abstract = {We investigate the automatic differentiation of dominant eigensolver where only a small proportion of eigenvalues and corresponding eigenvectors are obtained. Back-propagation through the dominant eigensolver involves solving certain low-rank linear systems without direct access to the full spectrum of the problem. Furthermore, the backward pass can be conveniently differentiated again, which implies that in principle one can obtain arbitrarily higher-order derivatives of the dominant eigendecomposition process. These results allow for the construction of an efficient dominant eigensolver primitive, which has wide applications in quantum physics. As a demonstration, we compute second-order derivative of the ground-state energy and fidelity susceptibility of one-dimensional transverse-field Ising model through the exact diagonalization approach. We also calculate the ground-state energy of the same model in the thermodynamic limit by performing gradient-based optimization of uniform matrix product states. By programming these computational tasks in a fully differentiable way, one can efficiently handle the dominant eigendecomposition of very large matrices while still sharing various advantages of differentiable programming paradigm, notably, the generic nature of the implementation and free of tedious human efforts of deriving gradients analytically.},
  file = {/Users/liujinguo/Zotero/storage/PJCL6T2W/Xie et al. - 2020 - Automatic differentiation of dominant eigensolver .pdf}
}

@article{Liao2019,
  title = {Differentiable {{Programming Tensor Networks}}},
  author = {Liao, Hai-jun and Liu, Jin-guo and Wang, Lei and Xiang, Tao},
  year = {2019},
  journal = {Physical Review X},
  volume = {9},
  number = {3},
  pages = {31041},
  publisher = {American Physical Society},
  issn = {2160-3308},
  doi = {10.1103/PhysRevX.9.031041},
  keywords = {computational physics,condensed,doi:10.1103/PhysRevX.9.031041 url:https://doi.org/},
  file = {/Users/liujinguo/Zotero/storage/UUB6BI64/Liao et al_2019_Differentiable Programming Tensor Networks.pdf}
}

@article{Zhang2023,
  title = {Automatic Differentiable {{Monte Carlo}}: {{Theory}} and Application},
  shorttitle = {Automatic Differentiable {{Monte Carlo}}},
  author = {Zhang, Shi-Xin and Wan, Zhou-Quan and Yao, Hong},
  year = {2023},
  month = jul,
  journal = {Physical Review Research},
  volume = {5},
  number = {3},
  pages = {033041},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevResearch.5.033041},
  urldate = {2024-06-10},
  abstract = {Differentiable programming has emerged as a key programming paradigm empowering rapid developments of deep learning while its applications to important computational methods such as Monte Carlo remain largely unexplored. Here we present the general theory enabling infinite-order automatic differentiation on expectations computed by Monte Carlo with unnormalized probability distributions, which we call automatic differentiable Monte Carlo (ADMC). By implementing ADMC algorithms on computational graphs, one can also leverage state-of-the-art machine learning frameworks and techniques in traditional Monte Carlo applications in statistics and physics. We illustrate the versatility of ADMC by showing some applications: fast search of phase transitions and accurately finding ground states of interacting many-body models in two dimensions. ADMC paves a promising way to innovate Monte Carlo in various aspects to achieve higher accuracy and efficiency.},
  file = {/Users/liujinguo/Zotero/storage/CVRNIQVA/Zhang et al. - 2023 - Automatic differentiable Monte Carlo Theory and a.pdf;/Users/liujinguo/Zotero/storage/97UDTK9E/PhysRevResearch.5.html}
}

@article{Griewank1992,
  title = {Achieving Logarithmic Growth of Temporal and Spatial Complexity in Reverse Automatic Differentiation},
  author = {Griewank, Andreas},
  year = {1992},
  journal = {Optimization Methods and Software},
  volume = {1},
  number = {1},
  pages = {35--54},
  issn = {10294937},
  doi = {10.1080/10556789208805505},
  abstract = {In its basic form the reverse mode of automatic differentiation yields gradient vectors at a small multiple of the computational work needed to evaluate the underlying scalar function. The practical applicability of this temporal complexity result, due originally to Linnainmaa, seemed to be severely limited by the fact that the memory requirement of the basic implementation is proportional to the run time, T, of the original evaluation program, It is shown here that, by a recursive scheme related to the multilevel differentiation approach of Volin and Ostrovskii, the growth in both temporal and spatial complexity can be limited to a fixed multiple of log(T). Other compromises between the run time and memory requirement are possible, so that the reverse mode becomes applicable to computational problems of virtually any size. {\copyright} 1992, Taylor \& Francis Group, LLC. All rights reserved.},
  keywords = {Adjoint,Checkpointing,Complexity,Gradient,Recursion},
  file = {/Users/liujinguo/Zotero/storage/9ALU8UD4/Griewank_1992_Achieving logarithmic growth of temporal and spatial complexity in reverse.pdf}
}

@article{Liu2021,
  title = {{Automatic differentiation and its applications in physics simulation}},
  author = {{Jin-Guo}, Liu and {Kai-Lai}, Xu},
  year = {2021},
  month = jul,
  journal = {物理学报},
  volume = {70},
  number = {14},
  pages = {149402--11},
  publisher = {物理学报},
  issn = {1000-3290},
  doi = {10.7498/aps.70.20210813},
  urldate = {2023-02-19},
  abstract = {Automatic differentiation is a technology to differentiate a computer program automatically. It is known to many people for its use in machine learning in recent decades. Nowadays, researchers are becoming increasingly aware of its importance in scientific computing, especially in the physics simulation. Differentiating physics simulation can help us solve many important issues in chaos theory, electromagnetism, seismic and oceanographic. Meanwhile, it is also challenging because these applications often require a lot of computing time and space. This paper will review several automatic differentiation strategies for physics simulation, and compare their pros and cons. These methods include adjoint state methods, forward mode automatic differentiation, reverse mode automatic differentiation, and reversible programming automatic differentiation.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {chinese},
  file = {/Users/liujinguo/Zotero/storage/76F3Y4A5/Jin-Guo_Kai-Lai_2021_Automatic differentiation and its applications in physics simulation.pdf}
}

@article{Seeger2017,
  title = {Auto-{{Differentiating Linear Algebra}}},
  author = {Seeger, Matthias and Hetzel, Asmus and Dai, Zhenwen and Meissner, Eric and Lawrence, Neil D},
  year = {2017},
  eprint = {1710.08717},
  abstract = {Development systems for deep learning (DL), such as Theano, Torch, TensorFlow, or MXNet, are easy-to-use tools for creating complex neural network models. Since gradient computations are automatically baked in, and execution is mapped to high performance hardware, these models can be trained end-to-end on large amounts of data. However, it is currently not easy to implement many basic machine learning primitives in these systems (such as Gaussian processes, least squares estimation, principal components analysis, Kalman smoothing), mainly because they lack efficient support of linear algebra primitives as differentiable operators. We detail how a number of matrix decompositions (Cholesky, LQ, symmetric eigen) can be implemented as differentiable operators. We have implemented these primitives in MXNet, running on CPU and GPU in single and double precision. We sketch use cases of these new operators, learning Gaussian process and Bayesian linear regression models, where we demonstrate very substantial reductions in implementation complexity and running time compared to previous codes. Our MXNet extension allows end-to-end learning of hybrid models, which combine deep neural networks (DNNs) with Bayesian concepts, with applications in advanced Gaussian process models, scalable Bayesian optimization, and Bayesian active learning.},
  archiveprefix = {arXiv},
  file = {/Users/liujinguo/Zotero/storage/67ACV2Q8/Seeger et al_2017_Auto-Differentiating Linear Algebra.pdf}
}

@misc{Hubig2019,
  title = {Use and Implementation of Autodifferentiation in Tensor Network Methods with Complex Scalars},
  author = {Hubig, Claudius},
  year = {2019},
  month = sep,
  number = {arXiv:1907.13422},
  eprint = {1907.13422},
  primaryclass = {cond-mat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.13422},
  urldate = {2025-02-01},
  abstract = {Following the recent preprints arXiv:1903.09650 and arXiv:1906.04654 we comment on the feasibility of implementation of autodifferentiation in standard tensor network toolkits by briefly walking through the steps to do so. The total implementation effort comes down to fewer than 1000 lines of additional code. We furthermore summarise the current status when the method is applied to cases where the underlying scalars are complex, not real and the final result is a real-valued scalar. It is straightforward to generalise most operations (addition, tensor products and also the QR decomposition) to this case and after the initial submission of these notes, also the adjoint of the complex SVD has been found.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Strongly Correlated Electrons},
  file = {/Users/liujinguo/Zotero/storage/8WF6QFFD/Hubig - 2019 - Use and implementation of autodifferentiation in tensor network methods with complex scalars.pdf;/Users/liujinguo/Zotero/storage/NA7JS8LK/1907.html}
}

@techreport{Townsend2016,
  title={Differentiating the singular value decomposition},
  author={Townsend, James},
  year={2016},
  institution={Technical Report 2016, https://j-towns. github. io/papers/svd-derivative~…}
}

@article{Giles2008,
  title={An extended collection of matrix derivative results for forward and reverse mode automatic differentiation},
  author={Giles, Mike},
  year={2008},
  publisher={Unspecified}
}

@misc{Wan2019,
  title = {Automatic {{Differentiation}} for {{Complex Valued SVD}}},
  author = {Wan, Zhou-Quan and Zhang, Shi-Xin},
  year = {2019},
  month = nov,
  number = {arXiv:1909.02659},
  eprint = {1909.02659},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.02659},
  urldate = {2025-02-01},
  abstract = {In this note, we report the back propagation formula for complex valued singular value decompositions (SVD). This formula is an important ingredient for a complete automatic differentiation(AD) infrastructure in terms of complex numbers, and it is also the key to understand and utilize AD in tensor networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Numerical Analysis,Condensed Matter - Statistical Mechanics,Condensed Matter - Strongly Correlated Electrons,Mathematics - Numerical Analysis,Quantum Physics,Statistics - Machine Learning},
  file = {/Users/liujinguo/Zotero/storage/CWTNXGI5/Wan and Zhang - 2019 - Automatic Differentiation for Complex Valued SVD.pdf}
}
